{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code File",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3VA4r9p2muGyh6l3O3VlE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "SNJ3vgFlkJya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGbTUBVqj9QL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "def preprocess(array):\n",
        "    \"\"\"\n",
        "    Normalizes the supplied array and reshapes it into the appropriate format.\n",
        "    \"\"\"\n",
        "\n",
        "    array = array.astype(\"float32\") / 255.0\n",
        "    array = np.reshape(array, (len(array), 28, 28, 1))\n",
        "    return array\n",
        "\n",
        "\n",
        "def noise(array):\n",
        "    \"\"\"\n",
        "    Adds random noise to each image in the supplied array.\n",
        "    \"\"\"\n",
        "\n",
        "    noise_factor = 0.4\n",
        "    noisy_array = array + noise_factor * np.random.normal(\n",
        "        loc=0.0, scale=1.0, size=array.shape\n",
        "    )\n",
        "\n",
        "    return np.clip(noisy_array, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def display(array1, array2):\n",
        "    \"\"\"\n",
        "    Displays ten random images from each one of the supplied arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    n = 10\n",
        "\n",
        "    indices = np.random.randint(len(array1), size=n)\n",
        "    images1 = array1[indices, :]\n",
        "    images2 = array2[indices, :]\n",
        "\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(image1.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(image2.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparing Data**"
      ],
      "metadata": {
        "id": "MCs_gAlZkREK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we only need images from the dataset to encode and decode, we\n",
        "# won't use the labels.\n",
        "(train_data, _), (test_data, _) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape the data\n",
        "train_data = preprocess(train_data)\n",
        "test_data = preprocess(test_data)\n",
        "\n",
        "# Create a copy of the data with added noise\n",
        "noisy_train_data = noise(train_data)\n",
        "noisy_test_data = noise(test_data)\n",
        "\n",
        "# Display the train data and a version of it with added noise\n",
        "display(train_data, noisy_train_data)"
      ],
      "metadata": {
        "id": "k-PlnsqzkUJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building the autoencoder**"
      ],
      "metadata": {
        "id": "zjV_SUuRkbjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "\n",
        "# Decoder\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder = Model(input, x)\n",
        "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "53qIpI1qkeEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Training Autoencoder**"
      ],
      "metadata": {
        "id": "3_y0K5s-kij1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(\n",
        "    x=train_data,\n",
        "    y=train_data,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    validation_data=(test_data, test_data),\n",
        ")"
      ],
      "metadata": {
        "id": "ozsZQ6t3ksDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction on Test Dataset**"
      ],
      "metadata": {
        "id": "Rt728Lrfkz8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = autoencoder.predict(test_data)\n",
        "display(test_data, predictions)"
      ],
      "metadata": {
        "id": "1bDLvChAk6hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-Training Using Noisy Data**"
      ],
      "metadata": {
        "id": "wL_tZmnsk87a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(\n",
        "    x=noisy_train_data,\n",
        "    y=train_data,\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    validation_data=(noisy_test_data, test_data),\n",
        ")"
      ],
      "metadata": {
        "id": "ZgaIiyV_lCTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction on Noisy Data**"
      ],
      "metadata": {
        "id": "Pqt5H8sXlFaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = autoencoder.predict(noisy_test_data)\n",
        "display(noisy_test_data, predictions)"
      ],
      "metadata": {
        "id": "nRtp5HcRlI5T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}